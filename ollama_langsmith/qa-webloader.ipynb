{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23213"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content = docs[0].page_content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "llm = ChatOllama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_db = Chroma.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vector_store_db.as_retriever(k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create_stuff_documents_chain helper function to \"stuff\" all of the input documents into the prompt. It will also handle formatting the docs as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the user input question based only on the provided contex. Make sure to keep the answer concise.\n",
    "\n",
    "<question>\n",
    "{input}\n",
    "</question>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), config={'run_name': 'format_inputs'})\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the user input question based only on the provided contex. Make sure to keep the answer concise.\\n\\n<question>\\n{input}\\n</question>\\n\\n<context>\\n{context}\\n</context>\\n'))])\n",
       "| ChatOllama(model='llama3.1')\n",
       "| StrOutputParser(), config={'run_name': 'stuff_documents_chain'})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The text appears to be a summary of an article about using Low-Rank Adaptation (LoRA) for fine-tuning large language models (LLMs). The article discusses the performance and efficiency of LoRA in various scenarios, including:\\n\\n* Fine-tuning LLMs on small datasets\\n* Comparing LoRA with full finetuning and Reinforcement Learning from Human Feedback (RLHF)\\n* Combining multiple sets of LoRA weights for model adaptation\\n* Exploring the possibility of layer-wise optimal rank adaptation\\n\\nThe article also mentions a personal project, \"Machine Learning with PyTorch and Scikit-Learn\", which provides practical tips for fine-tuning LLMs using LoRA.\\n\\nTo answer your original question, it seems that the text is discussing various aspects of using LoRA for fine-tuning large language models. However, I couldn\\'t find a specific section or quote that mentions \"Llama\" (a hypothetical AI assistant).'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain.invoke(\n",
    "    {\n",
    "        \"question\":\"Does LoRA Need to Be Enabled for All Layers?\",\n",
    "        \"context\":documents\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x10b9abf10>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the user input question based only on the provided contex. Make sure to keep the answer concise.\\n\\n<question>\\n{input}\\n</question>\\n\\n<context>\\n{context}\\n</context>\\n'))])\n",
       "            | ChatOllama(model='llama3.1')\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_retrieval_chain(retriver,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrival_chain = create_retrieval_chain(retriver,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Does LoRA Need to Be Enabled for All Layers ?',\n",
       " 'context': [Document(metadata={'description': 'Things I Learned From Hundreds of Experiments', 'language': 'en', 'source': 'https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms', 'title': 'Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)'}, page_content=\"model parameters are, of course, distributed across different matrices in many layers, but for simplicity, we refer to a single weight matrix here). During backpropagation, we learn a ΔW matrix, which contains information on how much we want to update the original weights to minimize the loss function during training.The weight update is then as follows:Wupdated = W + ΔWIf the weight matrix W contains 7B parameters, then the weight update matrix ΔW also contains 7B parameters, and computing the matrix ΔW can be very compute and memory intensive.The LoRA method proposed by Hu et al. replaces to decompose the weight changes, ΔW, into a lower-rank representation. To be precise, it does not require to explicitly compute ΔW. Instead, LoRA learns the decomposed representation of ΔW directly during training which is where the savings are coming from, as shown in the figure below.As illustrated above, the decomposition of ΔW means that we represent the large matrix ΔW with two smaller LoRA matrices, A and B. If A has the same number of rows as ΔW and B has the same number of columns as ΔW, we can write the decomposition as ΔW = AB. (AB is the matrix multiplication result between matrices A and B.)\\xa0How much memory does this save? It depends on the rank r, which is a hyperparameter. For example, if ΔW has 10,000 rows and 20,000 columns, it stores 200,000,000 parameters. If we choose A and B with r=8, then A has 10,000 rows and 8 columns, and B has 8 rows and 20,000 columns, that's 10,000×8 + 8×20,000 = 240,000 parameters, which is about 830× less than 200,000,000.Of course, A and B can't capture all the information that ΔW could capture, but this is by design. When using LoRA, we hypothesize that the model requires W to be a large matrix with full rank to capture all the knowledge in the pretraining dataset. However, when we finetune an LLM, we don't need to update all the weights and capture the core information for the adaptation in a smaller number of weights than ΔW would; hence, we have the low-rank updates via AB.1.\"),\n",
       "  Document(metadata={'description': 'Things I Learned From Hundreds of Experiments', 'language': 'en', 'source': 'https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms', 'title': 'Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)'}, page_content='SubscribeSign inShare this postPractical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)magazine.sebastianraschka.comCopy linkFacebookEmailNoteOtherPractical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)Things I Learned From Hundreds of ExperimentsSebastian Raschka, PhDNov 19, 2023213Share this postPractical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)magazine.sebastianraschka.comCopy linkFacebookEmailNoteOther40ShareLow-rank adaptation (LoRA) is among the most widely used and effective techniques for efficiently training custom LLMs. For those interested in open-source LLMs, it\\'s an essential technique worth familiarizing oneself with.Last month, I shared an article with several LoRA experiments, based on the open-source Lit-GPT repository that I co-maintain with my colleagues at Lightning AI. This Ahead of AI article aims to discuss the primary lessons I derived from my experiments. Additionally, I\\'ll address some of the frequently asked questions related to the topic.\\xa0 If you are interested in finetuning custom LLMs, I hope these insights will save you some time in \"the long run\" (no pun intended).In brief, the main takeaways I am discussing in this article are the following:Despite the inherent randomness of LLM training (or when training models on GPUs in general), the outcomes remain remarkably consistent across multiple runs.QLoRA presents a trade-off that might be worthwhile if you\\'re constrained by GPU memory. It offers 33% memory savings at the cost of a 39% increase in runtime.When finetuning LLMs, the choice of optimizer shouldn\\'t be a major concern. While SGD on its own is suboptimal, there\\'s minimal variation in outcomes whether you employ AdamW, SGD with a scheduler, or AdamW with a scheduler.While Adam is often labeled a memory-intensive optimizer due to its introduction of two new parameters for every model parameter, this doesn\\'t significantly affect the peak memory demands of the LLM. This is because the majority of the memory is allocated for large matrix'),\n",
       "  Document(metadata={'description': 'Things I Learned From Hundreds of Experiments', 'language': 'en', 'source': 'https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms', 'title': 'Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)'}, page_content=\"the majority of the memory is allocated for large matrix multiplications rather than retaining extra parameters.For static datasets, iterating multiple times, as done in multi-epoch training, might not be beneficial. It often deteriorates the results, probably due to overfitting.If you're incorporating LoRA, ensure it's applied across all layers, not just to the Key and Value matrices, to maximize model performance.Adjusting the LoRA rank is essential, and so is selecting an apt alpha value. A good heuristic is setting alpha at twice the rank's value.7 billion parameter models can be finetuned efficiently within a few hours on a single GPU possessing 14 GB of RAM. With a static dataset, optimizing an LLM to excel across all benchmark tasks is unattainable. Addressing this requires diverse data sources, or perhaps LoRA might not be the ideal tool.In addition, I will answer ten common questions around LoRA:Q1: How Important is the Dataset?Q2: Does LoRA Work for Domain Adaptation?Q3: How Do You Select the Best Rank?Q4: Does LoRA Need to Be Enabled for All Layers?Q5: How To Avoid Overfitting?Q6: What about Other Optimizers?Q7: What Other Factors Influence Memory Usage?Q8: How Does it Compare to Full Finetuning and RLHF?Q9: Can LoRA Weights be Combined?Q10: What about Layer-wise Optimal Rank Adaptation?(In the previous issue of AI, I mentioned that I wanted to write a more general introduction with a from-scratch code implementation of LoRA sometime if there's interest. According to your feedback, there's a lot of interest, and I plan to share another article on LoRA in the future. For now, this article is focused on the broader ideas and takeaways from working with LoRA—a top-down view.)A Brief Introduction to LoRALarge language models are large, and it can be expensive to update all model weights during training due to GPU memory limitations.\\xa0For example, suppose we have an LLM with 7B parameters represented in a weight matrix W. (In reality, the model parameters are, of course, distributed across different\"),\n",
       "  Document(metadata={'description': 'Things I Learned From Hundreds of Experiments', 'language': 'en', 'source': 'https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms', 'title': 'Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)'}, page_content=\"weight += (lora_B @ lora_A) * scaling As we can see in the code formula above, the larger the influence of the LoRA weights.Previous experiments used r=8 and alpha=16, which resulted in a 2-fold scaling. Choosing alpha as two times r is a common rule of thumb when using LoRA for LLMs, but I was curious if this still holds for larger r values. In other words, “alpha = 2×rank” really seems to be a sweet spot. However, in this specific combination of model and dataset, where r=256 and alpha=128 (a 0.5-fold scaling) performance is even better.(I experimented with r=32, r=64, r=128, and r=512 but omitted the results for clarity as r=256 resulted in the best performance.)Choosing alpha as two times as large as r may often result in the best outcomes, but it may also not hurt to experiment with different ratios.8. Training 7B Parameter Models on a Single GPUOne of the main takeaways is that LoRA allows us to finetune 7B parameter LLMs on a single GPU. In this particular case, using QLoRA with the best setting (r=256 and alpha=512) requires 17.86 GB with AdamW and takes about 3 hours (on an A100) for 50k training examples (here, the Alpaca dataset).In the remaining sections of this article, I am answering additional questions you might have.Answers to Common QuestionsQ1: How Important is the Dataset?The dataset can be critical. I used the Alpaca dataset, which contains 50k training examples, for my experiments. I chose this dataset because it's quite popular, and experimenting with different datasets was out of scope due to the already extensive length of the article.However, it's worth noting that Alpaca is a synthetic dataset that was generated by querying an old version of ChatGPT and is probably not the best by today's standards.\\xa0Data quality can be very important. For example, in June, I discussed the LIMA dataset (Ahead of AI #9: LLM Tuning & Dataset Perspectives), a curated dataset consisting of only 1k examples.According to the LIMA: Less Is More for Alignment paper, a 65B Llama model finetuned on LIMA\")],\n",
       " 'answer': 'No. According to the text, \"If you\\'re incorporating LoRA, ensure it\\'s applied across all layers, not just to the Key and Value matrices, to maximize model performance.\"'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival_chain.invoke(\n",
    "    {\n",
    "        \"input\":\"Does LoRA Need to Be Enabled for All Layers ?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
