{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ChatMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model='llama-3.1-8b-instant', api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"LangChain is an open-source Python library for building large language models and conversational AI systems. It provides a framework for connecting to various LLMs (Large Language Models) and enables you to create complex applications, such as chatbots, virtual assistants, and more.\\n\\nWhat do you want to learn or achieve with LangChain? Are you:\\n\\n1. New to LangChain and looking for a getting-started guide?\\n2. Trying to integrate LangChain with a specific LLM or API?\\n3. Building a specific application, such as a chatbot or virtual assistant?\\n4. Interested in exploring advanced topics, like multimodal interaction or knowledge graph embeddings?\\n\\nLet me know, and I'll do my best to help you get started or provide guidance on your project!\", response_metadata={'token_usage': {'completion_tokens': 155, 'prompt_tokens': 18, 'total_tokens': 173, 'completion_time': 0.206666667, 'prompt_time': 0.004918372, 'queue_time': 0.009076317, 'total_time': 0.211585039}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None}, id='run-562e272c-f65f-4167-9736-54905cbc5578-0', usage_metadata={'input_tokens': 18, 'output_tokens': 155, 'total_tokens': 173})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\n",
    "    [\n",
    "        HumanMessage('Hi ! I am learning langchain.')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You didn't mention what you were specifically learning about LangChain. Would you like to share what you're currently looking into? I'd be happy to help or offer guidance if you need it.\", response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 116, 'total_tokens': 156, 'completion_time': 0.053333333, 'prompt_time': 0.027001989, 'queue_time': 0.0016247809999999988, 'total_time': 0.080335322}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-8e3b8a49-601d-49c2-a893-f5713424a713-0', usage_metadata={'input_tokens': 116, 'output_tokens': 40, 'total_tokens': 156})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\n",
    "    [\n",
    "        HumanMessage('Hi ! I am learning langchain.'),\n",
    "        AIMessage(content=\"LangChain is a powerful tool for building AI applications. What specific aspect of LangChain are you learning about? LLMs (Large Language Models) or building Chain models with LangChain?\\n\\nIf you're just starting out, I can offer some general guidance or point you in the direction of some resources to get you started.\\n\\nAlso, are you working on a specific project or just exploring the possibilities of LangChain?\"),\n",
    "        HumanMessage('What was i learning ?')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session Management: In chatbot creation, each user session (or conversation) needs to have its own separate history. This ensures that when a user returns or continues a conversation, the bot can remember previous interactions, providing a seamless and context-aware experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Concurrent Sessions: If multiple users are interacting with the chatbot simultaneously, session_id helps in distinguishing between their respective conversations, preventing any mix-up of messages or histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_store = {}\n",
    "\n",
    "def get_session_history(session_id : str) -> BaseChatMessageHistory:\n",
    "    if session_id not in id_store:\n",
    "        id_store[session_id] = ChatMessageHistory()\n",
    "    return id_store[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = {\"configurable\":{\"session_id\":\"user1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_msg_history = RunnableWithMessageHistory(llm, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "| RunnableBranch(branches=[(RunnableBinding(bound=RunnableLambda(_is_not_async), config={'run_name': 'RunnableWithMessageHistoryInAsyncMode'}), RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x1145f3eb0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1145f31c0>, model_name='llama-3.1-8b-instant', groq_api_key=SecretStr('**********')), config_factories=[<function Runnable.with_alisteners.<locals>.<lambda> at 0x113ca7b80>]))], default=RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x1145f3eb0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1145f31c0>, model_name='llama-3.1-8b-instant', groq_api_key=SecretStr('**********')), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x1145fae50>])), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x11468b670>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_msg_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a powerful library for building conversational AI models. Runnables are a fundamental concept in LangChain, allowing you to execute tasks, routines, and complex logic in a modular and composable way. What specifically would you like to know about runnables in LangChain? Are you looking for:\n",
      "\n",
      "1. **Basic usage**: How to create and use runnables in your LangChain project?\n",
      "2. **Design patterns**: Best practices and common design patterns for implementing runnables?\n",
      "3. **Integration with other LangChain components**: How to use runnables in conjunction with other LangChain modules, such as chains, executors, or models?\n",
      "4. **Advanced topics**: How to handle errors, implement retries, or use runnables with async/await in LangChain?\n",
      "\n",
      "Let me know, and I'll be happy to help you learn more about runnables in LangChain!\n"
     ]
    }
   ],
   "source": [
    "response = with_msg_history.invoke(\n",
    "    [HumanMessage('Hi i am learning runnables in langchain.')],\n",
    "    config=user1\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You were learning about **Runnables in LangChain**.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_msg_history.invoke(\n",
    "    [HumanMessage('What was i learning about ?')],\n",
    "    config=user1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi i am learning runnables in langchain.\n",
      "AI: LangChain is a powerful library for building conversational AI models. Runnables are a fundamental concept in LangChain, allowing you to execute tasks, routines, and complex logic in a modular and composable way. What specifically would you like to know about runnables in LangChain? Are you looking for:\n",
      "\n",
      "1. **Basic usage**: How to create and use runnables in your LangChain project?\n",
      "2. **Design patterns**: Best practices and common design patterns for implementing runnables?\n",
      "3. **Integration with other LangChain components**: How to use runnables in conjunction with other LangChain modules, such as chains, executors, or models?\n",
      "4. **Advanced topics**: How to handle errors, implement retries, or use runnables with async/await in LangChain?\n",
      "\n",
      "Let me know, and I'll be happy to help you learn more about runnables in LangChain!\n",
      "Human: What was i learning about ?\n",
      "AI: You were learning about **Runnables in LangChain**.\n"
     ]
    }
   ],
   "source": [
    "print(id_store['user1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if user2 has access to user1 history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2 = {\"configurable\":{\"session_id\":\"user2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This is the beginning of our conversation, so we haven't discussed anything yet. What would you like to talk about? I can summarize our conversation at the end if you'd like.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_msg_history.invoke(\n",
    "    [HumanMessage('What was i learning about ?')],\n",
    "    config=user2\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['user1', 'user2'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_store.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
